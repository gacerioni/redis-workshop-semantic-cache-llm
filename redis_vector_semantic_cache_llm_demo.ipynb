{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlzcnB0jHO3L3p3XcGL4YY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gacerioni/redis-workshop-semantic-cache-llm/blob/master/redis_vector_semantic_cache_llm_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop - Redis as a VectorDB - Semantic Caching (RedisVL)\n",
        "\n",
        "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "\n",
        "Bem-vind[ao]s ao Workshop! Vamos ter uma experiência hands-on sobre como usar o Redis para Semantic Caching, integrando-se tranquilamente com a sua stack de LLM.\n",
        "\n",
        "\n",
        "Para uma experiência premium, como a que eu quero que vocês tenham, recomendo MUITO utilizar o Redis Insight (App ou Web) pra apoiar na visualização dos dados.\n",
        "\n",
        "https://redis.com/redis-enterprise/redis-insight/"
      ],
      "metadata": {
        "id": "ogK2OCgztLrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crie uma conta free forever no Redis Cloud\n",
        "\n",
        "Para criar a sua conta grátis no Redis Cloud, basta seguir este colab [aqui](https://https://github.com/gacerioni/redis-workshop-notebook-validator/blob/master/redis-workshop-setup-notebook-validator.ipynb).\n",
        "\n",
        "Clique no botão \"Open in colab\" e siga o passo a passo."
      ],
      "metadata": {
        "id": "bnYOac6E0S9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução\n",
        "\n",
        "O client **RedisVL** fornece uma interface de **Semantic Cache** que utiliza as capacidades de cache internas do Redis e o vector search para armazenar respostas de perguntas já respondidas anteriormente.\n",
        "\n",
        "Isso reduz o número de requisições e tokens enviados para serviços de LLM, diminuindo os custos e aumentando o throughput da aplicação ao reduzir o tempo necessário para gerar respostas em linguagem natural.\n",
        "\n",
        "Este colab vai te ensinar como usar o Redis como um cache semântico para suas aplicações."
      ],
      "metadata": {
        "id": "HHFnl9vL1pIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on: hora de começar a codar"
      ],
      "metadata": {
        "id": "l8aBQZZJ1Uh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Vamos instalar algumas dependências aqui mesmo, direto no colab."
      ],
      "metadata": {
        "id": "t4yBq1pp1cXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalando algumas deps que iremos usar no lab\n",
        "!pip install openai redisvl sentence-transformers"
      ],
      "metadata": {
        "id": "HtxRXrZPwSWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E vamos definir algumas constantes para o nosso lab, apenas pra evitar repetições:"
      ],
      "metadata": {
        "id": "BD8gaVi-SXvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import redis\n",
        "\n",
        "# Create a Redis connection using redis-py\n",
        "REDIS_FULL_URL = \"redis://default:blabla@redis-16962.c11.us-east-1-2.ec2.cloud.redislabs.com:16962\"\n",
        "r = redis.from_url(REDIS_FULL_URL)\n",
        "\n",
        "# Flush the entire Redis database\n",
        "r.flushall()\n",
        "\n",
        "print(\"Redis database flushed.\")\n",
        "\n",
        "\n",
        "# also, export the same as an env var\n",
        "!export REDIS_FULL_URL=\"redis://default:blabla@redis-16962.c11.us-east-1-2.ec2.cloud.redislabs.com:16962\""
      ],
      "metadata": {
        "id": "Qti5rbyUSi-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando o esqueleto para interceptar o prompt do usuário\n",
        "\n",
        "Neste bloco, vamos definir como iremos interagir com o LLM, de maneira bem simples.\n"
      ],
      "metadata": {
        "id": "r0zR5e3l2cxA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CpMqs-hsxHC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "import time\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
        "\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def ask_openai(question: str) -> str:\n",
        "    response = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=question,\n",
        "      max_tokens=200\n",
        "    )\n",
        "    return response.choices[0].text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, vamos fazer uma pergunta bem simples e direta, pra só depois começar o drift."
      ],
      "metadata": {
        "id": "dr7rjFsD5W4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_openai(\"What is the capital of Brazil?\"))"
      ],
      "metadata": {
        "id": "nmGGrcOiwg2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicializando o SemanticCache\n",
        "\n",
        "Ao ser inicializado, o SemanticCache criará automaticamente um índice dentro do Redis para o conteúdo do cache semântico."
      ],
      "metadata": {
        "id": "jmmm8Go05uxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from redisvl.extensions.llmcache import SemanticCache\n",
        "\n",
        "llmcache = SemanticCache(\n",
        "    name=\"llmcache\",                     # underlying search index name\n",
        "    prefix=\"llmcache\",                   # redis key prefix for hash entries\n",
        "    redis_url=REDIS_FULL_URL,  # redis connection url string\n",
        "    distance_threshold=0.1               # semantic cache distance threshold\n",
        ")"
      ],
      "metadata": {
        "id": "uXTF6DDgycxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O RedisVL compila uma CLI localmente para que você possa administrar o cache como um todo.\n",
        "\n",
        "Rode o comando a seguir, apenas pra gente ver que não tem nada muito interessante acontecendo com o `lmcache` (SemanticCache que nomeamos) ainda."
      ],
      "metadata": {
        "id": "5WgfN6ez0X2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rvl index info -i llmcache --url $REDIS_FULL_URL"
      ],
      "metadata": {
        "id": "IY4_hX2ezfNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uso Básico do Cache com o Redis"
      ],
      "metadata": {
        "id": "QtgmeZketpSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos guardar a pergunta numa variável `question`:"
      ],
      "metadata": {
        "id": "qGrtbqgxtuH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the capital of Brazil?\""
      ],
      "metadata": {
        "id": "tr8z9w4j0hB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E vamos checar se alguém já fez essa pergunta, semanticamente falando, permitindo um `distance_threshold=0`.\\\n",
        "*Nós passamos este threshold quando criamos o objeto SemanticCache neste lab.*"
      ],
      "metadata": {
        "id": "QrCJmLHEtzEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the semantic cache -- should be empty\n",
        "if response := llmcache.check(prompt=question):\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"Empty cache\")"
      ],
      "metadata": {
        "id": "BL4DB6y41YFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A verificação inicial do cache deve estar vazia, pois você ainda não armazenou nada no cache.\n",
        "\n",
        "Abaixo, armazene a pergunta, a resposta correta e quaisquer metadados arbitrários (como um objeto dicionário em Python) no cache."
      ],
      "metadata": {
        "id": "erc5JlmpvXTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache the question, answer, and arbitrary metadata\n",
        "llmcache.store(\n",
        "    prompt=question,\n",
        "    response=\"Brasília\",\n",
        "    metadata={\"city\": \"Brasília\", \"country\": \"brazil\", \"most_nerdola_citizen\": \"gabs\"}\n",
        ")\n"
      ],
      "metadata": {
        "id": "P3haiqKy1jaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E agora vamos testar de novo! Como a pergunta é a mesma, com certeza vamos ter o famoso cache hit, só que semântico."
      ],
      "metadata": {
        "id": "IFbTyDSHvoeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the cache again\n",
        "if response := llmcache.check(prompt=question, return_fields=[\"prompt\", \"response\", \"metadata\"]):\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"Empty cache\")"
      ],
      "metadata": {
        "id": "j1wtYTnA1tDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos fazer um teste simples aqui, só pra ver se o modelo conseguiu fazer o mínimo esperado para o nosso lab:"
      ],
      "metadata": {
        "id": "nTboaTdSv5wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for a semantically similar result\n",
        "question = \"What actually is the capital of Brazil?\"\n",
        "llmcache.check(prompt=question)[0]['response']"
      ],
      "metadata": {
        "id": "py4_IWd51ytP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customize o threshold de distância\n",
        "\n",
        "Na maioria dos casos de uso, o limite correto de similaridade semântica não é algo fixo.\n",
        "\n",
        "Dependendo da escolha do modelo de embeddings, das propriedades da consulta de entrada e do caso de uso de negócio, o limite pode precisar ser ajustado.\n",
        "\n",
        "Felizmente, você pode ajustar o limite de forma simples a qualquer momento, como mostrado abaixo:"
      ],
      "metadata": {
        "id": "Ub_pBBp-wFtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Widen the semantic distance threshold\n",
        "llmcache.set_threshold(0.3)"
      ],
      "metadata": {
        "id": "BM48FwyR12zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, nós podemos fazer uma busca meio enrolada, mas que ainda assim deixa claro o que queremos. Algo que caberia dentro desses `.3` de deviation que permitimos agora."
      ],
      "metadata": {
        "id": "03Y3fMglxMoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Really try to trick it by asking around the point\n",
        "# But is able to slip just under our new threshold\n",
        "question = \"What is the capital city of the country in LATAM that also has a city named São Paulo?\"\n",
        "llmcache.check(prompt=question)[0]['response']"
      ],
      "metadata": {
        "id": "skAplLv413_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos dar uma limpada no cache semântico agora, mas sem flushes destrutivos.\n",
        "\n",
        "A lib do RedisVL permite que façamos isso de maneira limpa e apenas nas entidades que queremos afetar."
      ],
      "metadata": {
        "id": "b6_RaCqjxbN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invalidate the cache completely by clearing it out\n",
        "llmcache.clear()\n",
        "\n",
        "# should be empty now\n",
        "llmcache.check(prompt=question)"
      ],
      "metadata": {
        "id": "19S6TEDf2DPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando TTL\n",
        "\n",
        "\n",
        "O Redis utiliza políticas (opcionais) de time-to-live (TTL) para expirar chaves individuais em momentos específicos no futuro. Isso permite que você se concentre no fluxo de dados e na lógica de negócio sem se preocupar com tarefas complexas de limpeza.\n",
        "\n",
        "Uma política de TTL configurada no SemanticCache permite que você mantenha temporariamente as entradas de cache.\n",
        "\n",
        "Por exemplo, defina a política de TTL para 5 segundos:"
      ],
      "metadata": {
        "id": "ML9ETCWSxraM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llmcache.set_ttl(5) # 5 seconds"
      ],
      "metadata": {
        "id": "Xh2p4E1K2Q4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fazendo um store simples, é certo que sua chave irá durar apenas os 5 segundos que definimos no passo anterior:"
      ],
      "metadata": {
        "id": "kJPiHwQA1BZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llmcache.store(\"This is a TTL test\", \"This is a TTL test response\")\n",
        "\n",
        "time.sleep(5) # sleep for 5 secs, so we don't GET"
      ],
      "metadata": {
        "id": "uOT4a5to2TyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm that the cache has cleared by now on it's own\n",
        "result = llmcache.check(\"This is a TTL test\")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "aRcmsDxR2Y1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos dar um reset no TTL, pra gente poder continuar a guardar chaves long-lived:"
      ],
      "metadata": {
        "id": "8OkAPjqJ3UXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the TTL to null (long lived data)\n",
        "llmcache.set_ttl()"
      ],
      "metadata": {
        "id": "j-glPoTf2nJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "P0JVf00h2s8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste Simples de Performance\n",
        "\n",
        "Em seguida, vamos medir o ganho de velocidade obtido ao usar o SemanticCache.\n",
        "\n",
        "Você utilizará o módulo time para medir o tempo necessário para gerar respostas com e sem o SemanticCache."
      ],
      "metadata": {
        "id": "2OBnhjda4XVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question: str) -> str:\n",
        "    \"\"\"Helper function to answer a simple question using OpenAI with a wrapper\n",
        "    check for the answer in the semantic cache first.\n",
        "\n",
        "    Args:\n",
        "        question (str): User input question.\n",
        "\n",
        "    Returns:\n",
        "        str: Response.\n",
        "    \"\"\"\n",
        "    results = llmcache.check(prompt=question)\n",
        "    if results:\n",
        "        return results[0][\"response\"]\n",
        "    else:\n",
        "        answer = ask_openai(question)\n",
        "        return answer"
      ],
      "metadata": {
        "id": "f0028j2E2t5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos ver o que acontece quando vamos no openAI direto? A latência não é ruim, mas deveria ser bem acima de um cache hit, certo?"
      ],
      "metadata": {
        "id": "8p674xCM4rfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "# asking a question -- openai response time\n",
        "question = \"What was the name of the first US President?\"\n",
        "answer = answer_question(question)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Without caching, a call to openAI to answer this simple question took {end-start} seconds.\")\n"
      ],
      "metadata": {
        "id": "yz4TaTwn20Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llmcache.store(prompt=question, response=\"George Washington\")"
      ],
      "metadata": {
        "id": "a5I6NDu625AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the avg latency for caching over LLM usage\n",
        "times = []\n",
        "\n",
        "for _ in range(10):\n",
        "    cached_start = time.time()\n",
        "    cached_answer = answer_question(question)\n",
        "    cached_end = time.time()\n",
        "    times.append(cached_end-cached_start)\n",
        "\n",
        "avg_time_with_cache = np.mean(times)\n",
        "print(f\"Avg time taken with LLM cache enabled: {avg_time_with_cache}\")\n",
        "print(f\"Percentage of time saved: {round(((end - start) - avg_time_with_cache) / (end - start) * 100, 2)}%\")\n"
      ],
      "metadata": {
        "id": "nVBf4O0E2-Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver uma foto sumarizada pelo CLI também:"
      ],
      "metadata": {
        "id": "cFbr0Mb97AsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rvl stats -i llmcache --url $REDIS_FULL_URL"
      ],
      "metadata": {
        "id": "k3aNw7zE7E5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear the cache\n",
        "llmcache.clear()"
      ],
      "metadata": {
        "id": "wPU7bUa13Eft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gerenciando um fluxo mais realista\n",
        "\n",
        "**Vamos fazer uma função inteligente, mas ainda assim super simples.**\n",
        "\n",
        "Basicamente, a função faz isso:\n",
        "- Recebe a pergunta como input;\n",
        "- Verifica se a pergunta está no SemanticCache que criamos;\n",
        "- Se achou (cache hit), responde imediatamente;\n",
        "- Se não achou (cache miss), vamos pra api do LLM + guardamos o dado no nosso SemanticCache.\n",
        "\n",
        "Desta forma, temos um fluxo bem básico, mas que já poderia estar num caminho de cliente."
      ],
      "metadata": {
        "id": "dsGRUYg14BFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question: str) -> str:\n",
        "    \"\"\"Helper function to answer a simple question using OpenAI with a wrapper\n",
        "    check for the answer in the semantic cache first. If not found, it queries\n",
        "    OpenAI and stores the response in the cache.\n",
        "\n",
        "    Args:\n",
        "        question (str): User input question.\n",
        "\n",
        "    Returns:\n",
        "        str: Response.\n",
        "    \"\"\"\n",
        "    # Check if the answer is already in the semantic cache\n",
        "    results = llmcache.check(prompt=question)\n",
        "\n",
        "    if results:\n",
        "        # If found, print message and return the cached response\n",
        "        print(\"[CACHE HIT] The answer was found in the semantic cache.\")\n",
        "        return results[0][\"response\"]\n",
        "    else:\n",
        "        # Otherwise, ask the LLM (OpenAI) for the answer\n",
        "        print(\"[CACHE MISS] The answer was not in the cache. Querying OpenAI...\")\n",
        "        answer = ask_openai(question)\n",
        "\n",
        "        # Store the question and its answer in the semantic cache\n",
        "        print(\"[CACHE STORE] Storing the new response in the semantic cache.\")\n",
        "        llmcache.store(prompt=question, response=answer)\n",
        "\n",
        "        # Return the answer\n",
        "        return answer"
      ],
      "metadata": {
        "id": "DoBjD_4A4Cu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neste trecho, vou simular um prompt pra gente brincar com o fluxo completo.\n",
        "*Algo mais realista, porém, bem simples (como tudo no Redis).*"
      ],
      "metadata": {
        "id": "YqTSU9Gr9RaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main block to ask user for input and check the cache or query the LLM\n",
        "while True:\n",
        "    # Open a prompt for the user to ask a question\n",
        "    question = input(\"Enter your question (or 'exit' to stop): \")\n",
        "\n",
        "    # Break the loop if the user types 'exit'\n",
        "    if question.lower() == 'exit':\n",
        "        print(\"Exiting the demo.\")\n",
        "        break\n",
        "\n",
        "    # Get the answer using the cached system\n",
        "    answer = answer_question(question)\n",
        "\n",
        "    # Display the answer\n",
        "    print(f\"Answer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "e9XcoT9x4psD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on Avançado: Cache Access Controls, Tags & Filters\n",
        "\n",
        "Ao executar workloads complexos com aplicativos similares rodando ao mesmo tempo, ou ao lidar com múltiplos usuários, é importante manter os dados segregados.\n",
        "\n",
        "Com base no suporte do **RedisVL** para consultas complexas e híbridas, podemos marcar e filtrar as entradas de cache utilizando `filterable_fields` definidos de forma personalizada.\n",
        "\n",
        "Vamos armazenar os dados de múltiplos usuários no nosso cache, com prompts IDÊNTICOS, e garantir que retornemos apenas as informações corretas de cada usuário:"
      ],
      "metadata": {
        "id": "uN7lvEH17YOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally delete the cache if needed\n",
        "# private_cache.delete()  # Comment this out to avoid errors if index doesn't exist\n",
        "\n",
        "private_cache = SemanticCache(\n",
        "    name=\"private_cache\",                     # underlying search index name\n",
        "    redis_url=REDIS_FULL_URL,  # redis connection url string\n",
        "    distance_threshold=0.1,               # semantic cache distance threshold\n",
        "    filterable_fields=[{\"name\": \"user_id\", \"type\": \"tag\"}]\n",
        ")"
      ],
      "metadata": {
        "id": "P6VtiuQOEAkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "private_cache.store(\n",
        "    prompt=\"What is the phone number linked to my account?\",\n",
        "    response=\"The number on file is 123-555-0000\",\n",
        "    filters={\"user_id\": \"gabs\"},\n",
        ")\n",
        "\n",
        "private_cache.store(\n",
        "    prompt=\"What's the phone number linked in my account?\",\n",
        "    response=\"The number on file is 123-555-1111\",\n",
        "    filters={\"user_id\": \"bart\"},\n",
        ")"
      ],
      "metadata": {
        "id": "vyQ-8ObpAbdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos definir agora o SemanticCache contextual, para que a resposta use o user id como contexto e filtro."
      ],
      "metadata": {
        "id": "9oaP3NspBXyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from redisvl.query.filter import Tag\n",
        "\n",
        "# define user id filter\n",
        "user_id_filter = Tag(\"user_id\") == \"bart\"\n",
        "\n",
        "response = private_cache.check(\n",
        "    prompt=\"What is the phone number linked to my account?\",\n",
        "    filter_expression=user_id_filter,\n",
        "    num_results=2\n",
        ")\n",
        "\n",
        "print(f\"found {len(response)} entry \\n{response[0]['response']}\")"
      ],
      "metadata": {
        "id": "Y0QFdx-YBx1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Múltiplos `filterable_fields` podem ser definidos em um cache, e expressões complexas de filtro podem ser construídas para filtrar nesses campos, assim como nos campos padrão já presentes."
      ],
      "metadata": {
        "id": "6hjyWl5qMJn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally delete the cache if needed\n",
        "#complex_cache.delete()  # Comment this out to avoid errors if index doesn't exist\n",
        "\n",
        "\n",
        "complex_cache = SemanticCache(\n",
        "    name='account_data',\n",
        "    redis_url=REDIS_FULL_URL,  # redis connection url string\n",
        "    filterable_fields=[\n",
        "        {\"name\": \"user_id\", \"type\": \"tag\"},\n",
        "        {\"name\": \"account_type\", \"type\": \"tag\"},\n",
        "        {\"name\": \"account_balance\", \"type\": \"numeric\"},\n",
        "        {\"name\": \"transaction_amount\", \"type\": \"numeric\"}\n",
        "    ]\n",
        ")\n",
        "complex_cache.store(\n",
        "    prompt=\"what is my most recent checking account transaction under $100?\",\n",
        "    response=\"Your most recent transaction was for $75\",\n",
        "    filters={\"user_id\": \"abc\", \"account_type\": \"checking\", \"transaction_amount\": 75},\n",
        ")\n",
        "complex_cache.store(\n",
        "    prompt=\"what is my most recent savings account transaction?\",\n",
        "    response=\"Your most recent deposit was for $300\",\n",
        "    filters={\"user_id\": \"abc\", \"account_type\": \"savings\", \"transaction_amount\": 300},\n",
        ")\n",
        "complex_cache.store(\n",
        "    prompt=\"what is my most recent checking account transaction over $200?\",\n",
        "    response=\"Your most recent transaction was for $350\",\n",
        "    filters={\"user_id\": \"abc\", \"account_type\": \"checking\", \"transaction_amount\": 350},\n",
        ")\n",
        "complex_cache.store(\n",
        "    prompt=\"what is my checking account balance?\",\n",
        "    response=\"Your current checking account is $1850\",\n",
        "    filters={\"user_id\": \"abc\", \"account_type\": \"checking\"},\n",
        ")"
      ],
      "metadata": {
        "id": "9_oH4VNQIwlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Então, podemos fazer essas buscas com os pré-filtros que precisamos:"
      ],
      "metadata": {
        "id": "Gjt9MSnEPBqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from redisvl.query.filter import Num\n",
        "\n",
        "value_filter = Num(\"transaction_amount\") > 100\n",
        "account_filter = Tag(\"account_type\") == \"checking\"\n",
        "complex_filter = value_filter & account_filter\n",
        "\n",
        "# check for checking account transactions over $100\n",
        "complex_cache.set_threshold(0.3)\n",
        "response = complex_cache.check(\n",
        "    prompt=\"what is my most recent checking account transaction?\",\n",
        "    filter_expression=complex_filter,\n",
        "    num_results=5\n",
        ")\n",
        "print(f'found {len(response)} entry')\n",
        "print(response[0][\"response\"])"
      ],
      "metadata": {
        "id": "G4zKNdW9PGlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}